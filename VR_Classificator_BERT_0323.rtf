{\rtf1\ansi\ansicpg1252\cocoartf2513
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 import os\
import pandas as pd\
import PyPDF2\
from transformers import BertTokenizer, BertForSequenceClassification, AdamW\
import torch\
import numpy as np\
\
# Set up device for training\
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\
\
# Define start and end target words\
startTargetWords = ["recommendation", "lessons learned", "advice to planning authorities"]\
endTargetWords = ["reference", "appendix", "annex", "list of", "conclusion", "bibliography", "works cited", "introduction", "board member statements", "executive summary", "abbreviations and acronyms"]\
\
# Load Excel spreadsheet with factor data\
factors_df = pd.read_excel('factors.xlsx')\
report_to_factors = \{\}\
for col in factors_df.columns[1:]:\
    report_to_factors[col] = \{\}\
    for index, row in factors_df.iterrows():\
        if not pd.isna(row[col]):\
            report_to_factors[col][row['Report Name']] = row[col]\
\
# Define function to extract sections of text from a PDF file\
def extract_sections(pdf_file):\
    sections = []\
    current_section = ''\
    started = False\
    pdf_reader = PyPDF2.PdfFileReader(pdf_file)\
    for page_num in range(pdf_reader.getNumPages()):\
        page = pdf_reader.getPage(page_num)\
        text = page.extractText()\
        for line in text.split('\\n'):\
            if any(word in line.lower() for word in startTargetWords):\
                started = True\
            if started:\
                current_section += line + '\\n'\
                if any(word in line.lower() for word in endTargetWords):\
                    started = False\
                    sections.append(current_section)\
                    current_section = ''\
    return sections\
\
# Define function to preprocess text for BERT\
def preprocess_text(text):\
    return tokenizer.encode_plus(\
        text,\
        add_special_tokens=True,\
        max_length=512,\
        padding='max_length',\
        truncation=True,\
        return_attention_mask=True,\
        return_tensors='pt'\
    )\
\
# Define function to train BERT model\
def train_model(model, optimizer, train_dataloader, device):\
    model.train()\
    total_loss, total_accuracy = 0, 0\
    total_preds = []\
    for step, batch in enumerate(train_dataloader):\
        batch = tuple(t.to(device) for t in batch)\
        b_input_ids, b_input_mask, b_labels = batch\
        optimizer.zero_grad()\
        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\
        loss = outputs.loss\
        logits = outputs.logits\
        total_loss += loss.item()\
        loss.backward()\
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\
        optimizer.step()\
        logits = logits.detach().cpu().numpy()\
        label_ids = b_labels.to('cpu').numpy()\
        total_preds.append(logits)\
        total_accuracy += flat_accuracy(logits, label_ids)\
    avg_loss = total_loss / len(train_dataloader)\
    avg_accuracy = total_accuracy / len(train_dataloader)\
    total_preds = np.concatenate(total_preds, axis=0)\
    return avg_loss, avg_accuracy, total_preds\
\
# Define function to calculate accuracy\
def flat_accuracy(preds, labels):\
    pred_flat = np.argmax(preds, axis=1).flatten()\
    labels_flat = labels.flatten()\
    return np.sum(pred_flat == labels_flat) /\
}