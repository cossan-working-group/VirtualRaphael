{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1037c86f-bfc1-45a7-92e9-f413ab8f0592",
   "metadata": {
    "id": "1037c86f-bfc1-45a7-92e9-f413ab8f0592",
    "outputId": "cea16daa-2bd9-4924-ce48-beaa7a3f46dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sections and sentences extracted and saved to New Report Pre Processed\\v2_2023_Relatório de Investigação de Incidente - P-19 (versão para revisão da atual GESTÂO) (1).txt\n",
      "Text cleaned and saved to New Report Cleaned\\v2_2023_Relatório de Investigação de Incidente - P-19 (versão para revisão da atual GESTÂO) (1).txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ypb20167\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to Predictions/Predictions.txt\n"
     ]
    }
   ],
   "source": [
    "#Pre process the new report\n",
    "import fitz\n",
    "import re\n",
    "import os\n",
    "\n",
    "def extract_sections_and_sentences(pdf_path, start_phrases, end_phrases, keywords):\n",
    "    # Open the PDF file\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "    # Initialize variables to store extracted data\n",
    "    sections = []\n",
    "    sentences = []\n",
    "\n",
    "    # Iterate through each page of the PDF\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        page_text = page.get_text()\n",
    "        paragraphs = re.split(r'\\n|\\r\\n', page_text)\n",
    "\n",
    "        # Initialize variables for section extraction\n",
    "        current_section = \"\"\n",
    "        extracting_section = False\n",
    "\n",
    "        # Iterate through paragraphs to extract sections\n",
    "        for paragraph in paragraphs:\n",
    "            # Check for the start of a section\n",
    "            for start_phrase in start_phrases:\n",
    "                if start_phrase.lower() in paragraph.lower():\n",
    "                    current_section = paragraph\n",
    "                    extracting_section = True\n",
    "                    break\n",
    "\n",
    "            # Check for the end of a section\n",
    "            for end_phrase in end_phrases:\n",
    "                if end_phrase.lower() in paragraph.lower():\n",
    "                    current_section += \"\\n\" + paragraph  # Include the end phrase\n",
    "                    sections.append(current_section)\n",
    "                    current_section = \"\"\n",
    "                    extracting_section = False\n",
    "                    break\n",
    "\n",
    "            # If extracting a section, append to the current section\n",
    "            if extracting_section:\n",
    "                current_section += \"\\n\" + paragraph\n",
    "\n",
    "            # Extract sentences containing keywords\n",
    "            for sentence in re.split(r'(?<=[.!?])\\s', paragraph):\n",
    "                if any(keyword.lower() in sentence.lower() for keyword in keywords):\n",
    "                    sentences.append(sentence)\n",
    "\n",
    "    pdf_document.close()\n",
    "\n",
    "    # Combine sections and sentences\n",
    "    combined_text = \"\\n\\n\".join(sections + sentences)\n",
    "\n",
    "    return combined_text\n",
    "\n",
    "def process_pdfs_in_folder(folder_path, start_phrases, end_phrases, keywords, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(folder_path, pdf_file)\n",
    "        output_file = os.path.join(output_folder, pdf_file.replace('.pdf', '.txt'))\n",
    "\n",
    "        combined_text = extract_sections_and_sentences(pdf_path, start_phrases, end_phrases, keywords)\n",
    "\n",
    "        # Save the combined text to the output file\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(combined_text)\n",
    "\n",
    "        print(\"Sections and sentences extracted and saved to\", output_file)\n",
    "\n",
    "# Action:\n",
    "folder_path = 'New Report'\n",
    "start_phrases = [\"recommendation\", \"lessons learned\", \"advice to planning authorities\"]\n",
    "end_phrases = [\"reference\", \"appendix\", \"annex\", \"list of\", \"conclusion\", \"bibliography\", \"works cited\",\n",
    "               \"introduction\", \"board member statements\", \"executive summary\", \"abbreviations and acronyms\"]\n",
    "keywords = [\"he\", \"she\", \"they\", \"I\", \"user\", \"operator\", \"manager\", \"management\", \"team\", \"lead\", \"leader\",\n",
    "            \"inspector\", \"mechanic\", \"engineer\", \"driver\", \"pilot\", \"crew\", \"worker\", \"contractor\", \"operative\"]\n",
    "output_folder = 'New Report Pre Processed'\n",
    "process_pdfs_in_folder(folder_path, start_phrases, end_phrases, keywords, output_folder)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import os\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Join the words back into a cleaned text\n",
    "    cleaned_text = ' '.join(words)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def clean_text_files_in_folder(input_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    txt_files = [f for f in os.listdir(input_folder) if f.endswith('.txt')]\n",
    "\n",
    "    for txt_file in txt_files:\n",
    "        input_file_path = os.path.join(input_folder, txt_file)\n",
    "        output_file_path = os.path.join(output_folder, txt_file)\n",
    "\n",
    "        with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        cleaned_text = clean_text(text)\n",
    "\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(cleaned_text)\n",
    "\n",
    "        print(f\"Text cleaned and saved to {output_file_path}\")\n",
    "\n",
    "# Action:\n",
    "input_folder = 'New Report Pre Processed'\n",
    "output_folder = 'New Report Cleaned'\n",
    "\n",
    "clean_text_files_in_folder(input_folder, output_folder)\n",
    "\n",
    "#Begin classification\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the folder containing models\n",
    "models_folder = \"Models\"\n",
    "\n",
    "# Define the path to the extracted text file\n",
    "text_folder = \"New Report Cleaned\"\n",
    "text_file_path = None\n",
    "for filename in os.listdir(text_folder):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        text_file_path = os.path.join(text_folder, filename)\n",
    "        break\n",
    "with open(text_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Initialize a list to store predictions\n",
    "predictions = []\n",
    "\n",
    "# Iterate through models\n",
    "for model_name in os.listdir(models_folder):\n",
    "    # Check if the item in the folder is a directory\n",
    "    if os.path.isdir(os.path.join(models_folder, model_name)):\n",
    "        # Load the fine-tuned BERT model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(os.path.join(models_folder, model_name))\n",
    "\n",
    "        # Make predictions using the tokenized input\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "\n",
    "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "        # Store\n",
    "        predictions.append({\n",
    "            \"model_name\": model_name,\n",
    "            \"predicted_class\": predicted_class,\n",
    "            \"class_probabilities\": probabilities.tolist()[0]\n",
    "        })\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = \"Predictions/Predictions.txt\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for prediction in predictions:\n",
    "        output_file.write(f\"Model Name: {prediction['model_name']}\\n\")\n",
    "        output_file.write(f\"Predicted Class: {prediction['predicted_class']}\\n\")\n",
    "\n",
    "print(f\"Predictions saved to {output_file_path}\") #194 seconds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
