# -*- coding: utf-8 -*-
"""Virt_Raph_New_Report.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11NZLPAEiGHC-nilCMpSzdVqHN8ZQWatT
"""
import nltk
import os
from .modules import extract_sections_and_sentences, process_pdfs_in_folder
from .modules import clean_text, clean_text_files_in_folder
import os
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from tqdm import tqdm



### Pre process the new report ###


# Action:
folder_path = 'New Report'
start_phrases = ["recommendation", "lessons learned", "advice to planning authorities"]
end_phrases = ["reference", "appendix", "annex", "list of", "conclusion", "bibliography", "works cited",
               "introduction", "board member statements", "executive summary", "abbreviations and acronyms"]
keywords = ["he", "she", "they", "I", "user", "operator", "manager", "management", "team", "lead", "leader",
            "inspector", "mechanic", "engineer", "driver", "pilot", "crew", "worker", "contractor", "operative"]
output_folder = 'New Report Pre Processed'
process_pdfs_in_folder(folder_path, start_phrases, end_phrases, keywords, output_folder)


# Download NLTK stopwords
nltk.download('stopwords')


# Action:
input_folder = 'New Report Pre Processed'
output_folder = 'New Report Cleaned'

clean_text_files_in_folder(input_folder, output_folder)




### Begin classification ###

# Define the folder containing models
models_folder = "Models"

# Define the path to the extracted text file
text_folder = "New Report Cleaned"
text_file_path = None
for filename in os.listdir(text_folder):
    if filename.endswith(".txt"):
        text_file_path = os.path.join(text_folder, filename)
        break
with open(text_file_path, "r", encoding="utf-8") as file:
    text = file.read()

# Tokenize
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)

# Initialize a list to store predictions
predictions = []

# Iterate through models
for model_name in os.listdir(models_folder):
    # Check if the item in the folder is a directory
    if os.path.isdir(os.path.join(models_folder, model_name)):
        # Load the fine-tuned BERT model
        model = AutoModelForSequenceClassification.from_pretrained(os.path.join(models_folder, model_name))

        # Make predictions using the tokenized input
        with torch.no_grad():
            outputs = model(**inputs)

        probabilities = torch.softmax(outputs.logits, dim=1)

        predicted_class = torch.argmax(probabilities, dim=1).item()

        # Store
        predictions.append({
            "model_name": model_name,
            "predicted_class": predicted_class,
            "class_probabilities": probabilities.tolist()[0]
        })

# Define the output file path
output_file_path = "Predictions/Predictions.txt"

# Open the file in write mode
with open(output_file_path, "w", encoding="utf-8") as output_file:
    for prediction in predictions:
        output_file.write(f"Model Name: {prediction['model_name']}\n")
        output_file.write(f"Predicted Class: {prediction['predicted_class']}\n")

print(f"Predictions saved to {output_file_path}") #194 seconds