import fitz  # PyMuPDF
import re

# Define start target words
startTargetWords = ["recommendation", "lessons learned", "advice to planning authorities"]

# Define keywords for sentence extraction
keywords = ["he", "she", "they", "I", "user", "operator", "manager", "management", "team", "lead", "leader", "inspector", "mechanic", "engineer", "driver", "pilot", "crew", "worker", "contractor", "operative"]

# Function to extract text from a PDF file
def extract_text_from_pdf(pdf_file, start_page, end_page):
    doc = fitz.open(pdf_file)
    text = ""
    if end_page is None:
        end_page = len(doc)  # Set end_page to the number of pages in the document
    for page_num in range(start_page, end_page):  
        page = doc[page_num]
        text += page.get_text()
    return text

# Function to extract 20 lines following start target words and full sentences containing keywords
def extract_lines_and_sentences(text):
    output_lines = []
    found_target_word = False
    sentences = re.split(r'(?<=[.!?])\s+', text)  # Split text into sentences

    for sentence in sentences:
        # Check if sentence contains any of the keywords
        if any(keyword in sentence.lower() for keyword in keywords):
            output_lines.append(sentence.strip())

        # Check if sentence contains any of the start target words
        if any(word in sentence.lower() for word in startTargetWords):
            found_target_word = True

        # Extract the 40 lines following the start target words
        if found_target_word:
            output_lines.append(sentence.strip())

            # Break after 40 lines
            if len(output_lines) >= 40:
                break

    return output_lines

# Path to your PDF file
pdf_file_path = "example.pdf"

# Specify the starting and ending pages - skip contents, glossary, etc.
start_page = ##
end_page = None  # Set to None to process until the end of the document

# Extract text from the PDF
pdf_text = extract_text_from_pdf(pdf_file_path, start_page, end_page)

# Extract lines and sentences
output_lines = extract_lines_and_sentences(pdf_text)

# Save the extracted lines and sentences to a new text file
with open("Extracted.txt", "w") as output_file:
    output_file.write("\n".join(output_lines))

print("Extraction and saving complete.")




import transformers

# Define BART summarizer
tokenizer = transformers.BartTokenizer.from_pretrained("facebook/bart-large-cnn")
model = transformers.BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn")

# Function to summarize text using BART
def summarize_text(text):
    inputs = tokenizer.encode(text, return_tensors="pt", max_length=1024, truncation=True)
    summary_ids = model.generate(inputs, max_length=200, min_length=5, length_penalty=2.0, num_beams=4, early_stopping=True)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# Function to process and summarize long text
def process_long_text(long_text):
    # Split the long text into manageable sections (e.g., 5000 characters per section)
    section_length = 5000
    text_sections = [long_text[i:i + section_length] for i in range(0, len(long_text), section_length)]

    # Initialize a list to store summaries
    summaries = []

    # Process and summarize each section
    for section in text_sections:
        summary = summarize_text(section)
        summaries.append(summary)

    # Combine the summaries into one
    combined_summary = "\n".join(summaries)

    return combined_summary

# Load the extracted text
with open("Extracted.txt", "r") as text_file:
    extracted_text = text_file.read()

# Check if the extracted text is too long
if len(extracted_text) > 1024:
    summary = process_long_text(extracted_text)
else:
    summary = summarize_text(extracted_text)

# Save the summary to a text file
with open("summary.txt", "w") as summary_file:
    summary_file.write(summary)
