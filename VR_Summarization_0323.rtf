import fitz
import re

# Define start and end target words
startTargetWords = ["recommendation", "lessons learned", "advice to planning authorities"]
endTargetWords = ["reference", "appendix", "annex", "list of", "conclusion", "bibliography", "works cited", "introduction", "board member statements", "executive summary", "abbreviations and acronyms"]

# Define keywords for sentence extraction
keywords = ["he", "she", "they", "I", "user", "operator", "manager", "management", "team", "lead", "leader", "inspector", "mechanic", "engineer", "driver", "pilot", "crew", "worker", "contractor", "operative"]

# Function to extract relevant sections of text and full sentences containing keywords
def extract_text_and_sentences(pdf_path, output_file):
    # Open PDF file
    doc = fitz.open(pdf_path)

    # Initialize variables to store relevant text and sentences
    relevant_text = []
    relevant_sentences = []

    # Loop through pages in PDF
    for page in doc:
        # Extract text from page
        text = page.get_text()

        # Check for start and end target words
        for startTargetWord in startTargetWords:
            for endTargetWord in endTargetWords:
                start_index = text.lower().find(startTargetWord.lower())
                end_index = text.lower().find(endTargetWord.lower(), start_index)
                if start_index != -1 and end_index != -1:
                    # Add relevant text to the relevant_text list
                    relevant_text.append(text[start_index:end_index])

        # Split text into sentences
        sentences = re.split(r'(?<=[.!?])\s+', text)

        # Loop through sentences and find those containing keywords
        for sentence in sentences:
            if any(keyword.lower() in sentence.lower() for keyword in keywords):
                # Add sentences to the relevant_sentences list
                relevant_sentences.append(sentence)

    # Close PDF file
    doc.close()

    # Write relevant text and sentences to the output file
    with open(output_file, "w") as file:
        file.write("\n\n".join(relevant_text))
        file.write("\n\n".join(relevant_sentences))

# Example usage
pdf_path = "example.pdf"  # Replace with the path to your PDF file
output_file = "extracted_text.txt"  # Name of the output text file
extract_text_and_sentences(pdf_path, output_file)




import transformers

# Define BART summarizer
tokenizer = transformers.BartTokenizer.from_pretrained("facebook/bart-large-cnn")
model = transformers.BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn")

# Function to summarize text using BART
def summarize_text(text):
    inputs = tokenizer.encode(text, return_tensors="pt", max_length=1024, truncation=True)
    summary_ids = model.generate(inputs, max_length=200, min_length=5, length_penalty=2.0, num_beams=4, early_stopping=True)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# Function to process and summarize long text
def process_long_text(long_text):
    # Split the long text into manageable sections (e.g., 5000 characters per section)
    section_length = 5000
    text_sections = [long_text[i:i + section_length] for i in range(0, len(long_text), section_length)]

    # Initialize a list to store summaries
    summaries = []

    # Process and summarize each section
    for section in text_sections:
        summary = summarize_text(section)
        summaries.append(summary)

    # Combine the summaries into one
    combined_summary = "\n".join(summaries)

    return combined_summary

# Load the extracted text
with open("extracted_text.txt", "r") as text_file:
    extracted_text = text_file.read()

# Check if the extracted text is too long
if len(extracted_text) > 1024:
    summary = process_long_text(extracted_text)
else:
    summary = summarize_text(extracted_text)

# Save the summary to a text file
with open("summary.txt", "w") as summary_file:
    summary_file.write(summary)
