import fitz  # PyMuPDF
import re

# Define start target words
startTargetWords = ["recommendation", "lessons learned", "advice to planning authorities"]

# Define keywords for sentence extraction
keywords = ["he", "she", "they", "I", "user", "operator", "manager", "management", "team", "lead", "leader", "inspector", "mechanic", "engineer", "driver", "pilot", "crew", "worker", "contractor", "operative","copilot","co-pilot","captain",]

# Function to extract text from a PDF file
def extract_text_from_pdf(pdf_file, start_page, end_page):
    doc = fitz.open(pdf_file)
    text = ""
    if end_page is None:
        end_page = len(doc)  # Set end_page to the number of pages in the document
    for page_num in range(start_page, end_page):  
        page = doc[page_num]
        text += page.get_text()
    return text

# Function to extract 20 lines following start target words and full sentences containing keywords
def extract_lines_and_sentences(text):
    output_lines = []
    found_target_word = False
    sentences = re.split(r'(?<=[.!?])\s+', text)  # Split text into sentences

    for sentence in sentences:
        # Check if sentence contains any of the keywords
        if any(keyword in sentence.lower() for keyword in keywords):
            output_lines.append(sentence.strip())

        # Check if sentence contains any of the start target words
        if any(word in sentence.lower() for word in startTargetWords):
            found_target_word = True

        # Extract the 40 lines following the start target words
        if found_target_word:
            output_lines.append(sentence.strip())

            # Break after 40 lines
            if len(output_lines) >= 40:
                break

    return output_lines

# Path to your PDF file
pdf_file_path = "example.Paris.pdf"

# Specify the starting and ending pages - skip contents, glossary, etc.
start_page = ##
end_page = None  # Set to None to process until the end of the document

# Extract text from the PDF
pdf_text = extract_text_from_pdf(pdf_file_path, start_page, end_page)

# Extract lines and sentences
output_lines = extract_lines_and_sentences(pdf_text)

# Save the extracted lines and sentences to a new text file
with open("Extracted.txt", "w") as output_file:
    output_file.write("\n".join(output_lines))

print("Extraction and saving complete.")

!pip install transformers
from transformers import BartTokenizer, BartForConditionalGeneration
import os
import textwrap

# Load the BART tokenizer and model
tokenizer = BartTokenizer.from_pretrained("facebook/bart-large-cnn")
model = BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn")

# Read the input text file
input_file = "Extracted.txt"
with open(input_file, "r", encoding="utf-8") as file:
    input_text = file.read()

# Split the text into sections of 1000 words
section_length = 1000
sections = textwrap.wrap(input_text, section_length)

# Initialize variables
output_text = ""
remaining_sections = sections.copy()

# Summarize sections and combine them until the output is less than 1500 words
while len(output_text.split()) < 1500 and remaining_sections:
    # Get the next section
    current_section = remaining_sections.pop(0)

    # Summarize the section to a maximum of 200 tokens
    max_summary_length = 200
    inputs = tokenizer.encode("summarize: " + current_section, return_tensors="pt", max_length=1024, truncation=True)
    summary_ids = model.generate(inputs, max_length=max_summary_length, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    # Add the summarized section to the output
    output_text += summary + "\n"

# Save the summarized text as a text file
output_file = "summarized_output.txt"
with open(output_file, "w", encoding="utf-8") as file:
    file.write(output_text)

print("Summarization complete. Output saved to", output_file)

